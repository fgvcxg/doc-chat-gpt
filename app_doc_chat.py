import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader, UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.chains.question_answering import load_qa_chain
from langchain_openai import ChatOpenAI

# Load environment variables
load_dotenv()

# OpenAI API Key
api_key = os.getenv("OPENAI_API_KEY")

# ë¬¸ì„œ ë””ë ‰í† ë¦¬ ì„¤ì •
doc_dir = "docs"

# Streamlit UI
st.set_page_config(page_title="ğŸ“š ë¬¸ì„œ ê¸°ë°˜ GPT ì±—ë´‡", layout="wide")
st.title("ğŸ“š ë¡œì»¬ ë¬¸ì„œ ê¸°ë°˜ GPT ì±—ë´‡")

# ë¬¸ì„œ ìë™ ë¡œë“œ í•¨ìˆ˜
def load_documents():
    with st.spinner("ğŸ“ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        all_docs = []
        for file in os.listdir(doc_dir):
            if file.endswith(".pdf"):
                loader = PyPDFLoader(os.path.join(doc_dir, file))
                all_docs.extend(loader.load())
#            elif file.endswith(".pptx"):
#                loader = UnstructuredPowerPointLoader(os.path.join(doc_dir, file))
#                all_docs.extend(loader.load())
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        split_docs = splitter.split_documents(all_docs)
        vectordb = FAISS.from_documents(split_docs, OpenAIEmbeddings(openai_api_key=api_key))
        st.session_state["vectordb"] = vectordb
        st.session_state["ready"] = True

# ì•± ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ë¬¸ì„œ ë¡œë“œ
if "ready" not in st.session_state:
    load_documents()

# ì§ˆë¬¸ UI
if st.session_state.get("ready"):
    question = st.text_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
    if question:
        with st.spinner("ğŸ¤– GPTê°€ ë‹µë³€ ì¤‘ì…ë‹ˆë‹¤..."):
            docs = st.session_state["vectordb"].similarity_search(question, k=5)
            llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, openai_api_key=api_key)
            chain = load_qa_chain(llm, chain_type="stuff")
            response = chain.run(input_documents=docs, question=question)
            st.markdown("### ğŸ§  ë‹µë³€")
            st.write(response)
else:
    st.info("ğŸ”„ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤... ì§ˆë¬¸ì€ ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.")
