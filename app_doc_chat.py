import os
import streamlit as st
from dotenv import load_dotenv
from langchain.schema import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from pptx import Presentation
import pandas as pd
import docx

# âœ… .envì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# âœ… Streamlit ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="ë¬¸ì„œ ê¸°ë°˜ GPT ì±—ë´‡", layout="wide")
st.title("ğŸ“š ë¬¸ì„œ ê¸°ë°˜ ëŒ€í™”í˜• GPT ì±—ë´‡")

# ì•± ì‹¤í–‰ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
doc_dir = "docs"

# ì—†ìœ¼ë©´ ìë™ ìƒì„± (Streamlit Cloud ëŒ€ì‘)
if not os.path.exists(doc_dir):
    os.makedirs(doc_dir)

# âœ… ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸° ë²„íŠ¼
if st.button("âœ… ë¬¸ì„œ ë¡œë“œ ë° ì±—ë´‡ ì¤€ë¹„"):
    all_docs = []
    st.info("ğŸ“ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")

    for file in os.listdir(doc_dir):
        path = os.path.join(doc_dir, file)
        if file.endswith(".pdf"):
            loader = PyPDFLoader(path)
            all_docs.extend(loader.load())
        elif file.endswith(".pptx"):
            prs = Presentation(path)
            text = ""
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text += shape.text + "\n"
            all_docs.append(Document(page_content=text, metadata={"source": file}))
        elif file.endswith(".xlsx"):
            xls = pd.read_excel(path, sheet_name=None)
            text = ""
            for sheet_name, df in xls.items():
                text += f"[{sheet_name}]\n" + df.to_string(index=False) + "\n\n"
            all_docs.append(Document(page_content=text, metadata={"source": file}))
        elif file.endswith(".docx"):
            doc = docx.Document(path)
            text = "\n".join([p.text for p in doc.paragraphs])
            all_docs.append(Document(page_content=text, metadata={"source": file}))

    # âœ… ë¬¸ì„œ ì¡°ê° ë° ë²¡í„°í™”
    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    split_docs = splitter.split_documents(all_docs)
    vectordb = FAISS.from_documents(split_docs, OpenAIEmbeddings(openai_api_key=api_key))
    retriever = vectordb.as_retriever()

    # âœ… GPT-3.5 ì—°ê²°
    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=api_key),
        retriever=retriever,
        return_source_documents=True
    )
    st.session_state.qa_chain = qa_chain
    st.success("ë¬¸ì„œ ë¡œë”© ë° ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ!")

# âœ… ì§ˆë¬¸ ì…ë ¥ì°½ (ëŒ€í™”í˜• ìŠ¤íƒ€ì¼)
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if user_input and "qa_chain" in st.session_state:
    with st.spinner("GPTê°€ ë‹µë³€ ì¤‘ì…ë‹ˆë‹¤..."):
        result = st.session_state.qa_chain({"query": user_input})
        answer = result["result"]
        sources = result["source_documents"]

        # ğŸ’¬ ëŒ€í™” ë‚´ìš© ì €ì¥
        st.session_state.chat_history.append(("user", user_input))
        st.session_state.chat_history.append(("gpt", answer))

        # ğŸ§  ëŒ€í™” ë‚´ìš© ì¶œë ¥
        for role, msg in st.session_state.chat_history:
            if role == "user":
                st.chat_message("user").write(msg)  # ì˜¤ë¥¸ìª½ ì •ë ¬
            else:
                st.chat_message("assistant").write(msg)  # ì™¼ìª½ ì •ë ¬

        # ğŸ“ ì°¸ì¡° ë¬¸ì„œ í‘œì‹œ
        with st.expander("ğŸ“ ì°¸ì¡° ë¬¸ì„œ ë³´ê¸°"):
            for doc in sources:
                st.markdown(f"ğŸ“„ **{doc.metadata['source']}**")
                st.code(doc.page_content[:500] + "...")
