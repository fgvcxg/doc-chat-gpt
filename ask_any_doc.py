import os
from langchain.schema import Document
from langchain.vectorstores import Chroma
from langchain.embeddings import FakeEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from pptx import Presentation
import pandas as pd
import docx

# ëŒ€ìƒ í´ë”
doc_folder = "D:/toyproj"
all_docs = []

print("ğŸ“‚ ë¬¸ì„œ ë¡œë”© ì‹œì‘...")

# âœ… 1. PDF
for file in os.listdir(doc_folder):
    if file.endswith(".pdf"):
        loader = PyPDFLoader(os.path.join(doc_folder, file))
        all_docs.extend(loader.load())

# âœ… 2. PPTX
for file in os.listdir(doc_folder):
    if file.endswith(".pptx"):
        path = os.path.join(doc_folder, file)
        prs = Presentation(path)
        text = ""
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text += shape.text + "\n"
        all_docs.append(Document(page_content=text, metadata={"source": file}))

# âœ… 3. XLSX
for file in os.listdir(doc_folder):
    if file.endswith(".xlsx"):
        path = os.path.join(doc_folder, file)
        xls = pd.read_excel(path, sheet_name=None)  # ëª¨ë“  ì‹œíŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
        text = ""
        for sheet_name, df in xls.items():
            text += f"â–¶ ì‹œíŠ¸: {sheet_name}\n"
            text += df.to_string(index=False) + "\n\n"
        all_docs.append(Document(page_content=text, metadata={"source": file}))

# âœ… 4. DOCX
for file in os.listdir(doc_folder):
    if file.endswith(".docx"):
        path = os.path.join(doc_folder, file)
        doc = docx.Document(path)
        text = "\n".join([p.text for p in doc.paragraphs])
        all_docs.append(Document(page_content=text, metadata={"source": file}))

print(f"âœ… ì´ ë¬¸ì„œ ìˆ˜: {len(all_docs)}ê°œ")

# í…ìŠ¤íŠ¸ ë¶„í• 
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
split_docs = splitter.split_documents(all_docs)

# GPT ì—†ì´ FAKE ì„ë² ë”©ìœ¼ë¡œ ê²€ìƒ‰ì—”ì§„ êµ¬ì„±
embedding = FakeEmbeddings(size=768)
db = Chroma.from_documents(split_docs, embedding)
retriever = db.as_retriever()

# ì§ˆì˜ ì‘ë‹µ ë£¨í”„
print("\nğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‹œì‘ (ì¢…ë£Œí•˜ë ¤ë©´ 'exit')")
while True:
    query = input("\nì§ˆë¬¸ í‚¤ì›Œë“œ: ")
    if query.lower() in ["exit", "quit"]:
        print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
        break

    results = retriever.get_relevant_documents(query)

    if not results:
        print("âŒ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"\nğŸ“Œ ê´€ë ¨ ë¬¸ì„œ ì¡°ê° {len(results)}ê°œ:")
        for i, doc in enumerate(results):
            print(f"\n--- ë¬¸ì„œ {i+1} ({doc.metadata['source']}) ---")
            print(doc.page_content.strip()[:500] + "...")
